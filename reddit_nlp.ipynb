{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import praw\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tyiu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('words')\n",
    "\n",
    "# Get words from NLTK\n",
    "# words = set(nltk.corpus.words.words())\n",
    "# Get stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Lemmatizer and Stemmer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "# stemmer= PorterStemmer()\n",
    "# Get punctuation\n",
    "punct_list = [i for i in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "goog_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reddit login info\n",
    "reddit = praw.Reddit(client_id = 'W1R0o531tGoOtA',\n",
    "                     client_secret = '1Xeft_Tf9Vv-TkHSkPAMIUtF5Oc',\n",
    "                     user_agent = 'NLP Project (by /u/metisproj4)',\n",
    "                     user_name = 'metisproj4',\n",
    "                     password = 'Password12345')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call praw to grab stuff from Reddit\n",
    "reddits = ['investing', 'stocks', 'personalfinance', 'options', \n",
    "           'economics', 'financialindependence', 'stockmarket',\n",
    "           'marketing', 'SecurityAnalysis', 'AskReddit', 'askscience', \n",
    "           'explainlikeimfive', 'books', 'LifeProTips', \n",
    "           'technology', 'history', 'dating_advice', 'writing'\n",
    "          ]\n",
    "\n",
    "grabbed_list_title = []\n",
    "grabbed_list_body = []\n",
    "grabbed_list_comment1 = []\n",
    "grabbed_list_comment2 = []\n",
    "grabbed_list_comment3 = []\n",
    "grabbed_list_upvote_ratio = []\n",
    "\n",
    "for r in reddits:\n",
    "    sub_list_title = []\n",
    "    sub_list_body = []\n",
    "#     sub_list_upvote_ratio = []\n",
    "    \n",
    "    grabbed = reddit.subreddit(r).hot(limit=None)\n",
    "    for i in grabbed:\n",
    "        sub_list_title.append(i.title)\n",
    "        sub_list_body.append(i.selftext)\n",
    "#         sub_list_upvote_ratio.append(i.upvote_ratio)\n",
    "    \n",
    "    grabbed_list_title.append(sub_list_title)\n",
    "    grabbed_list_body.append(sub_list_body)\n",
    "#     grabbed_list_upvote_ratio.append(sub_list_upvote_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/tyiu/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean_text_lem(doc, words):\n",
    "    nuked = ' '.join(w for w in nltk.wordpunct_tokenize(doc) if w.lower() in words or w.istitle())\n",
    "    mid_clean = nuked.strip().lower()\n",
    "    for i in punct_list:\n",
    "        mid_clean = mid_clean.replace(i, '')\n",
    "    mid_clean = re.sub(r'\\d+', '', mid_clean)\n",
    "    cleaned = ''\n",
    "    for i in mid_clean.split():\n",
    "        if i not in stop_words:\n",
    "            cleaned = cleaned + ' ' + lemmatizer.lemmatize(i)\n",
    "    return cleaned.strip()\n",
    "\n",
    "# Get a lemmatized Reddit corpus\n",
    "corpus_lem_title = []\n",
    "corpus_lem_body = []\n",
    "pos_list = []\n",
    "\n",
    "for r_index in range(len(reddits)):\n",
    "    for row_index in range(len(grabbed_list_title[r_index])):\n",
    "        corpus_lem_title.append(clean_text_lem(grabbed_list_title[r_index][row_index], words))\n",
    "        corpus_lem_body.append(clean_text_lem(grabbed_list_body[r_index][row_index], words))\n",
    "        pos_list.append([r_index, row_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count vectorize and get TF and TF_IDF vectors\n",
    "cv=CountVectorizer(max_df=0.85)\n",
    "tf_vector=cv.fit_transform(corpus_lem_body)\n",
    "tf_feature_names = cv.get_feature_names()\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tf_idf_vector = tfidf_transformer.fit_transform(tf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_list = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_list.append(' '.join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        print('Topic %d:' % (topic_idx))\n",
    "        print(topic_list[topic_idx])\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For running LDA (topics not so great)\n",
    "# no_topics = 15\n",
    "\n",
    "# lda = LatentDirichletAllocation(n_components=no_topics)\n",
    "# lda.fit(tf_vector)\n",
    "\n",
    "# X_fitted_lda = lda.transform(tf_vector)\n",
    "\n",
    "# no_top_words = 7\n",
    "# display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run NMF (Seems better than LDA)\n",
    "no_topics = 30\n",
    "\n",
    "nmf = NMF(n_components=no_topics, alpha=.1, l1_ratio=.5, init='nndsvd', random_state=10)\n",
    "nmf.fit(tf_idf_vector)\n",
    "\n",
    "no_top_words = 20\n",
    "X_fitted_nmf = nmf.transform(tf_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "date really girl said guy relationship dating want told went go never ask friend met talking talk back going got\n",
      "Topic 1:\n",
      "stock price dividend portfolio trading top bought value invest global idea hold chart selling split resistance investor cap looking analysis\n",
      "Topic 2:\n",
      "posting post new see thread promotion rudeness relaxed excessive politics warrant apply self tend rule busy finance please consider top\n",
      "Topic 3:\n",
      "work get year job one much money could fire make also way life want need working even around lot help\n",
      "Topic 4:\n",
      "story character main plot world protagonist novel reader fantasy write want chapter first scene one idea series make person villain\n",
      "Topic 5:\n",
      "option price call put strike spread expiration sell position contract loss profit sold risk premium underlying trading selling close value\n",
      "Topic 6:\n",
      "earnings click billion revenue per share quarter chart week report release growth year refinitiv elite consensus last calendar today beat\n",
      "Topic 7:\n",
      "credit card score account bank debt use get spread debit balance limit discover pay one back report close got put\n",
      "Topic 8:\n",
      "marketing digital medium social business agency content experience internship ad work marketer job advice facebook analytics learn new product client\n",
      "Topic 9:\n",
      "fund account roth money index vanguard tax invest investment retirement mutual brokerage bank traditional cash put emergency currently portfolio employer\n",
      "Topic 10:\n",
      "would help invest greatly thanks recommend love also thought hear appreciate advice hi could possible thank helpful say advance war\n",
      "Topic 11:\n",
      "company business value share growth million revenue price tesla public another debt industry meat dividend based cap new cash offer\n",
      "Topic 12:\n",
      "financial relevant information house money need making next blackjack redditors tolerance registered rep strongly horizon invest employed proper exposure expensive\n",
      "Topic 13:\n",
      "like something feel way look seem find stuff maybe really even anything kind trying example much used talk use always\n",
      "Topic 14:\n",
      "writing write novel writer draft feel written advice really first short come plot get scene go word want voice problem\n",
      "Topic 15:\n",
      "anyone looking wondering thanks experience hi find else use advance recommend portfolio advice everyone help research share interested hey used\n",
      "Topic 16:\n",
      "day trading next today week every one swing hour back friday morning go question went trade lucky strategy text last\n",
      "Topic 17:\n",
      "think really going go everyone pretty happen bottom could ever interesting worth way meat war big anything personally safe basically\n",
      "Topic 18:\n",
      "long term short hold investment invest thinking low looking gain spy believe run holding strategy potential risk put interest make\n",
      "Topic 19:\n",
      "book read reading history one series first love really finish finished thread interesting weekly author everybody written thought also found\n",
      "Topic 20:\n",
      "trade china war trump deal chinese trading tariff economy apple threat week strategy article use dip billion often swing risk\n",
      "Topic 21:\n",
      "market cap open portfolio value index bull trading recession click year today average crash growth fed strategy high bond volatility\n",
      "Topic 22:\n",
      "know want dont let get please really question start anything find getting much history kind also learn curious place ask\n",
      "Topic 23:\n",
      "time first full part period hard long spend since spent done one ahead finding multiple lot every many going last\n",
      "Topic 24:\n",
      "buy sell want hold possible go say bought let wait right thinking selling dip call covered invest strategy broker instead\n",
      "Topic 25:\n",
      "io inc corp capital management group holding fund ltd co insider filing al counsel change income trust therapeutic partner service\n",
      "Topic 26:\n",
      "good idea bad thank looking bit watch better look list pretty starting start sub low dont free need everything thanks\n",
      "Topic 27:\n",
      "people many lot someone history thought dating world wondering great say find actually fire social make person ancient see meet\n",
      "Topic 28:\n",
      "car pay loan month debt interest payment home paying house rent monthly mortgage year insurance student rate income currently wife\n",
      "Topic 29:\n",
      "news search fstocks aautomoderator sort author new data daily yahoo fed link reuters corporate finance economic check article technical fundamental\n"
     ]
    }
   ],
   "source": [
    "topic_list = display_topics(nmf, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redditors\n",
      "fstocks\n",
      "aautomoderator\n"
     ]
    }
   ],
   "source": [
    "# Convert topics to word2vec representation\n",
    "topic_vectors = []\n",
    "for topic in topic_list:\n",
    "    row_vector = []\n",
    "    for w in topic.split():\n",
    "        try:\n",
    "            row_vector.append(goog_model.get_vector(w))\n",
    "        except:\n",
    "            print(w)\n",
    "    topic_vectors.append(row_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get mean word2vec representation for topics\n",
    "topic_2_vec = []\n",
    "for vec_list in topic_vectors:\n",
    "    vec_sum = np.zeros((300,))\n",
    "    for topic_vec in vec_list:\n",
    "        vec_sum = vec_sum + topic_vec\n",
    "    topic_2_vec.append(vec_sum*(1/len(vec_list)))\n",
    "    \n",
    "# goog_model.most_similar_cosmul(positive=preprocess_string(topic_list[0]))[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for i, val in enumerate(topic_2_vec):\n",
    "    topics.append(goog_model.similar_by_vector(val, topn=3, restrict_vocab=50000))\n",
    "    #print('Topic' + str(i))\n",
    "    #print(topics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change my reddit corpus for titles to word2vec\n",
    "error_check = 25\n",
    "debugging = False\n",
    "\n",
    "title_2_vec = []\n",
    "for title in corpus_lem_title:\n",
    "    vec_sum = np.zeros((300,))\n",
    "    for w in title.split():\n",
    "        try:\n",
    "            if debugging:\n",
    "                print(goog_model.get_vector(w)[error_check])\n",
    "            vec_sum = vec_sum + goog_model.get_vector(w)\n",
    "        except:\n",
    "            continue\n",
    "    if debugging:\n",
    "        print('\\n', vec_sum[error_check])\n",
    "    try:\n",
    "        title_2_vec.append(vec_sum*(1/len(title.split())))\n",
    "    except:\n",
    "        title_2_vec.append(np.zeros((300,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change my reddit corpus for body text to word2vec\n",
    "error_check = 25\n",
    "debugging = False\n",
    "\n",
    "body_2_vec = []\n",
    "for body in corpus_lem_body:\n",
    "    vec_sum = np.zeros((300,))\n",
    "    for w in body.split():\n",
    "        try:\n",
    "            if debugging:\n",
    "                print(goog_model.get_vector(w)[error_check])\n",
    "            vec_sum = vec_sum + goog_model.get_vector(w)\n",
    "        except:\n",
    "            continue\n",
    "    if debugging:\n",
    "        print('\\n', vec_sum[error_check])\n",
    "    try:\n",
    "        body_2_vec.append(vec_sum*(1/len(title.split())))\n",
    "    except:\n",
    "        body_2_vec.append(np.zeros((300,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list_clean = ['dating',                  #0 \n",
    "                    'stock dividends',         #1\n",
    "                    'submission/post rules',   #2\n",
    "                    'working and job',         #3\n",
    "                    'fantasy writing',         #4\n",
    "                    'options',                 #5\n",
    "                    'big company earnings',    #6\n",
    "                    'credit card debt',        #7\n",
    "                    'marketing careers advice',#8\n",
    "                    'financial planning',      #9\n",
    "                    'investment advice',       #10\n",
    "                    'growth company stocks',   #11\n",
    "                    'investment advice',       #12\n",
    "                    'random noise',            #13\n",
    "                    'writing advice',          #14\n",
    "                    'portfolio advice',        #15\n",
    "                    'day trading',             #16\n",
    "                    'general news',            #17\n",
    "                    'long term investing',     #18\n",
    "                    'books',                   #19\n",
    "                    'Trade War',               #20\n",
    "                    'stock crash recession',   #21\n",
    "                    'random noise',            #22\n",
    "                    'random noise',            #23\n",
    "                    'buy the dip',             #24\n",
    "                    'fund and legal',          #25\n",
    "                    'random ideas',            #26\n",
    "                    'social network dating',   #27\n",
    "                    'credit and loans',        #28\n",
    "                    'stock market news'        #29\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_topics(string):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    topic_weights = nmf.transform(cv.transform([clean_text_lem(string, words)]))\n",
    "    topic_w_df = pd.DataFrame()\n",
    "    topic_w_df['topic'] = topic_list_clean\n",
    "    topic_w_df['weights'] = topic_weights.T\n",
    "    return topic_w_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>random noise</td>\n",
       "      <td>0.173243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>big company earnings</td>\n",
       "      <td>0.031878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>options</td>\n",
       "      <td>0.030407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>growth company stocks</td>\n",
       "      <td>0.010521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>credit and loans</td>\n",
       "      <td>0.007302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    topic   weights\n",
       "13           random noise  0.173243\n",
       "6    big company earnings  0.031878\n",
       "5                 options  0.030407\n",
       "11  growth company stocks  0.010521\n",
       "28       credit and loans  0.007302"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_str_topics(corpus_lem_body[200]).sort_values(by='weights', ascending=False)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All of the popular S&P 500 ETFs like SPY, VOO, and IVV all cost over $250/share. Are there any S&P 500 ETFs or other options out there that are cheaper?'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grabbed_list_body[0][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2vec(string):\n",
    "    parsed_str = preprocess_string(string)\n",
    "    vec_sum = np.zeros((300,))\n",
    "    for w in parsed_str:\n",
    "        try:\n",
    "            vec_sum = vec_sum + goog_model.get_vector(w)\n",
    "        except:\n",
    "            continue\n",
    "    return vec_sum*(1/len(parsed_str))\n",
    "\n",
    "def get_closest_stuff(string, w2vec_matrix):\n",
    "    w2vec_string = get_w2vec(test_str)\n",
    "    index = []\n",
    "    cos_dist = []\n",
    "    for i, val in enumerate(w2vec_matrix):\n",
    "        index.append(i)\n",
    "        if len(val)==0:\n",
    "            cos_dist.append(1000)\n",
    "        else:\n",
    "            cos_dist.append(distance.cosine(w2vec_string, val))\n",
    "    cos_dist_df = pd.DataFrame()\n",
    "    cos_dist_df['index'] = index\n",
    "    cos_dist_df['cos_dist'] = cos_dist\n",
    "    return cos_dist_df\n",
    "\n",
    "def get_closest_stuff_detrended(detrended, w2vec_matrix):\n",
    "    w2vec_string = detrended\n",
    "    index = []\n",
    "    cos_dist = []\n",
    "    for i, val in enumerate(w2vec_matrix):\n",
    "        index.append(i)\n",
    "        if len(val)==0:\n",
    "            cos_dist.append(1000)\n",
    "        else:\n",
    "            cos_dist.append(distance.cosine(w2vec_string, val))\n",
    "    cos_dist_df = pd.DataFrame()\n",
    "    cos_dist_df['index'] = index\n",
    "    cos_dist_df['cos_dist'] = cos_dist\n",
    "    return cos_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sentiment of everything\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "sentiment_title = []\n",
    "sentiment_body = []\n",
    "for val in pos_list:\n",
    "    vs_title = analyzer.polarity_scores(grabbed_list_title[val[0]][val[1]])\n",
    "    sentiment_title.append(vs_title)\n",
    "    vs_body = analyzer.polarity_scores(grabbed_list_body[val[0]][val[1]])\n",
    "    sentiment_body.append(vs_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_posts(df, raw_list, num=10):\n",
    "    sorted_df = pd.DataFrame()\n",
    "    sorted_df = df.sort_values(by='cos_dist', ascending=True)[0:num].copy()\n",
    "    for i, val in enumerate(sorted_df['index']):\n",
    "        print(raw_list[pos_list[val][0]][pos_list[val][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_str = 'I hate stocks'\n",
    "# cos_dist_df = get_closest_stuff(input_str, title_2_vec)\n",
    "# print_posts(cos_dist_df, grabbed_list_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detrend title and body\n",
    "detrended_title = []\n",
    "for i, val in enumerate(corpus_lem_title):\n",
    "    if sentiment_title[i]['compound'] >= 0.10:\n",
    "        detrended_title.append(title_2_vec[i] - get_w2vec('great'))\n",
    "    elif sentiment_title[i]['compound'] <= 0.10:\n",
    "        detrended_title.append(title_2_vec[i] - get_w2vec('hate'))\n",
    "    else:\n",
    "        detrended_title.append(title_2_vec[i])\n",
    "        \n",
    "detrended_body = []\n",
    "for i, val in enumerate(corpus_lem_body):\n",
    "    if sentiment_body[i]['compound'] >= 0.10:\n",
    "        detrended_body.append(body_2_vec[i] - get_w2vec('great'))\n",
    "    elif sentiment_body[i]['compound'] <= 0.10:\n",
    "        detrended_body.append(body_2_vec[i] - get_w2vec('hate'))\n",
    "    else:\n",
    "        detrended_body.append(body_2_vec[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sentiment free w2vecs\n",
    "\n",
    "input_str = 'Stocks are the thing I love'\n",
    "sentiment_input = analyzer.polarity_scores(input_str)\n",
    "\n",
    "if sentiment_input['compound'] >= 0.10:\n",
    "    detrend_input_vec = get_w2vec(input_str) - get_w2vec('love')\n",
    "elif sentiment_input['compound'] <= 0.10:\n",
    "    detrend_input_vec = get_w2vec(input_str) - get_w2vec('hate')\n",
    "else:\n",
    "    detrend_input_vec = get_w2vec(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_results(input_vec, search_over_vec, print_from_list, num):\n",
    "    cos_dist_df = get_closest_stuff_detrended(input_vec, search_over_vec)\n",
    "    print_posts(cos_dist_df, print_from_list, num=num)\n",
    "    \n",
    "def biased_search_title(input_vec, bias, num):\n",
    "    if bias == 0:\n",
    "        return search_results(detrend_input_vec, detrended_title, grabbed_list_title, num)\n",
    "    elif bias > 0:\n",
    "        return search_results(detrend_input_vec+get_w2vec('love'), title_2_vec, grabbed_list_title, num)\n",
    "    elif bias < 0:\n",
    "        return search_results(detrend_input_vec+get_w2vec('hate'), title_2_vec, grabbed_list_title, num)\n",
    "    \n",
    "def biased_search_body(input_vec, bias, num):\n",
    "    if bias == 0:\n",
    "        return search_results(detrend_input_vec, detrended_body, grabbed_list_body, num)\n",
    "    elif bias > 0:\n",
    "        return search_results(detrend_input_vec+get_w2vec('love'), body_2_vec, grabbed_list_body, num)\n",
    "    elif bias < 0:\n",
    "        return search_results(detrend_input_vec+get_w2vec('hate'), body_2_vec, grabbed_list_body, num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks are down\n",
      "Stock wishlist?\n",
      "AQ stock buyout\n",
      "BYND Stock\n",
      "KHC Stock\n"
     ]
    }
   ],
   "source": [
    "biased_search_title(get_w2vec(input_str), bias=0, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
